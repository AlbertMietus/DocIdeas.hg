.. include:: /std/localtoc.irst

.. _ConcurrentComputingConcepts:

=============================
Concurrent Computing Concepts
=============================

.. post::
   :category: Castle DesignStudy
   :tags: Castle, Concurrency

   Shortly, more and more cores will become available alike I described in “:ref:`BusyCores`”. Castle should
   make it easy to write code for all of them: not to keep them busy, but to maximize speed up [useCase:
   :need:`U_ManyCore`].
   |BR|
   We also discussed threads_: they do not scale well for CPU-bound (embedded) systems. And I introduced some
   contemporary abstractions; which do not always fit nicely in existing languages.

   As Castle is a new language we have the opportunity to select such a concept and incorporate it into the language ...

   In this blog, we explore a bit of theory. I will focus on semantics and the possibilities to implement them
   efficiently. The exact syntax will come later.

Basic terminology
=================

As there is many theory available and even more practical expertise, but a limited set of “common words”, let describe
some basic terms. As always, we use Wikipedia as common ground, and add links for a deep-dive.
|BR|
Again, we use ‘task’ as the most generic term of work-to-be-executed; that can be (in) a process, (on) a thread, (by) a
computer, etc.


.. include:: CCC-sidebar-concurrency.irst

Concurrency
-----------

Concurrency_ is the ability to “compute” multiple *tasks* at the same time.
|BR|
Designing concurrent software isn’t that complicated but; demands another mindset the when we write software that does
one tasks afer the other.

A typical example is a loop: suppose we have a sequence of numbers and we like to compute the square of each one. Most
developers will loop over those numbers, get one number, calculate the square, store it in another list, and continue.
It works, but we have also instructed the computer to do it in sequence — especially when the
task is bit more complicated, the compiler does know whether the ‘next task’ depends on the current one, and can’t
optimise it.

A better plan is to tell the compiler about the tasks; most are independently: square a number. There is also one that
has to be run at the end: combine the results into a new list. And one is bit funny: distribute the sequence-elements
over the “square-tasks” — clearly, one has to start with this one, but it can be concurrent with many others too.
|BR|
This is *not* a parallel algorithm. When not specifying the order, we allow parallel execution. We do not demand it,
sequential execution is allowed too.


Parallelisme
------------

Parallelisme_ is about executing multiple tasks (apparently) at the same time. We will focus running multiple concurrent
task (of the same program) on “as many cores as possible”.  When we assume a thousand cores, we need a thousand
independent tasks (at least) to gain maximal speed up. A thousand at any moment!
|BR|
It’s not only about doing a thousand tasks at the same time (that is not to complicated, for a computer), but also —
probably: mostly — about finishing a thousand times faster…

With many cores, multiple program-steps can be executed at the same time: from changing the same variable,  acces the
same memory, or compete for new memory. And when solving that, we introduce new hazards: like deadlocks_ and even
livelocks_.


Distributed
~~~~~~~~~~~

A special form of parallelisme is Distributed-Computing_: compute on many computers. Many experts consider this
as an independent field of expertise; still --as Multi-Core_ is basically “many computers on a chips”-- its there is an
analogy [#DistributedDiff]_, and we should use the know-how that is available, to design out “best ever language”.



Communication
-------------

When tasks run in various environments they have to communicate: to pass data and to controll progress. Unlike in a
sequential program -- where the controll is trivial, as sharing data-- this needs a bit of extra effort.
|BR|
There are two main approches: shared-data of message-passing.

Shared Memory
~~~~~~~~~~~~~
In this model all tasks (thread or process) have some shared/common memory; typically “variables”. As the acces is
asynchronous, the risk exist the data is updated “at the same time” by two or more tasks. This can lead to invalid data;
and so Critical-Sections_ are needed.

This is a very basic model which assumes that there is physically memory that can be shared. In distributed systems this
is uncommon; but for threads it’s straightforward. As disadvantage of this model is that is hazardous: Even when a
single access to such a shared variable is not protected by a Critical-Section_, the whole system can break [#OOCS]_.


Messages
~~~~~~~~
A more modern approach is Message-Passing_. One task send a message (sometimes called “event”) to another task; as there
is a distinct sender and receiver -- and apparently no common/shared memory-- no Critical-Sections [#MPCS]_ are
needed. At least no explicitly. Messages can be used by all kind of task; even in a distributed system -- then the
message (and it data) is serialised, transmitted over a network and deserialised. Which can introduce some overhead and
delay.
|BR|
Many people use this networking mental model when they thing about Message-Passing_, and *wrongly* assume there is
always overhead. When (carefully) implemented this is not needed; and can be as efficiently as shared-memory (assuming
there is shared-memory to can be used).


************************************************************

.. todo:: All below is draft and needs work!!!!



Models
======

Probably the oldest model to described concurrency is the Petri-net_; which has the disadvantage that is synchronous
(all tokens move at the same timeslot) -- which is a hard to implement (efficiently) on Multi-Core_.

Actors

Actor-Model_
Actor-Model-Theory_

A very introduce

--------

END

----------

.. rubric:: Footnotes

.. [#DistributedDiff]
   There a two (main) differences between Distributed-Computing_ and Multi-Core_. Firstly, all “CPUs” in
   Distributed-Computing_ are active, independent and asynchronous. There is no option to share a “core” (as
   commonly/occasionally done in Multi-process/Threaded programming); nor is there “shared memory” (one can only send
   messages over a network).
   |BR|
   Secondly, collaboration with (network based) messages is a few orders slower then (shared) memory communication. This
   makes it harder to speed-up; the delay of messaging shouldn't be bigger as the acceleration do doing thing in
   parallel.
   |BR|
   But that condition does apply to Multi-Core_ too. Although the (timing) numbers do differ.

.. [#OOCS]
   The brittleness of Critical-Sections_ can be reduced by embedding (the) (shared-) variable in an OO abstraction. By
   using *getters and *setters*, that controll the access, the biggest risk is (mostly) gone. That does not, however,
   prevent deadlocks_ nor livelocks_. Also see the note below.

.. [#MPCS]
   This is not completely correct; Message-Passing_ can be implemented on top of shared-memory. Then, the implementation
   of this (usually) OO-abstraction contains the Critical-Sections_; a bit as described in the footnote above.



.. _pthreads: 			https://en.wikipedia.org/wiki/Pthreads
.. _Threads:			https://en.wikipedia.org/wiki/Thread_(computing)
.. _Multi-Core:			https://en.wikipedia.org/wiki/Multi-core_processor

.. _deadlocks:			https://en.wikipedia.org/wiki/Deadlock
.. _livelocks:			https://en.wikipedia.org/wiki/Deadlock#Livelock
.. _Critical-Section:		https://en.wikipedia.org/wiki/Critical_section
.. _Critical-Sections:		Critical-Section_
.. _Distributed-Computing:	https://en.wikipedia.org/wiki/Distributed_computing
.. _Message-Passing:		https://en.wikipedia.org/wiki/Message_passing
.. _Actor-Model:		https://en.wikipedia.org/wiki/Actor_model
.. _Actor-Model-Theory:		https://en.wikipedia.org/wiki/Actor_model_theory

.. _Petri-Net:			https://en.wikipedia.org/wiki/Petri_net
