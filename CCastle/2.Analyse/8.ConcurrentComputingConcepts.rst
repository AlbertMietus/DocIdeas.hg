.. include:: /std/localtoc.irst

.. _ConcurrentComputingConcepts:

=============================
Concurrent Computing Concepts
=============================

.. post::
   :category: Castle DesignStudy
   :tags: Castle, Concurrency

   Shortly, more and more cores will become available alike I described in “:ref:`BusyCores`”. Castle should
   make it easy to write code for all of them: not to keep them busy, but to maximize speed up [useCase:
   :need:`U_ManyCore`].
   |BR|
   We also discussed threads_: they do not scale well for CPU-bound (embedded) systems. And I introduced some
   contemporary abstractions; which do not always fit nicely in existing languages.

   As Castle is a new language we have the opportunity to select such a concept and incorporate it into the language ...

   In this blog, we explore a bit of theory. I will focus on semantics and the possibilities to implement them
   efficiently. The exact syntax will come later.

Basic terminology
=================

As there is many theory available and even more practical expertise, but a limited set of “common words”, let describe
some basic terms. As always, we use Wikipedia as common ground, and add links for a deep-dive.

.. include:: CCC-sidebar-concurrency.irst



TODO
************************************************************

.. todo:: All below is draft and needs work!!!!



Concurrency
-----------

Concurrency_ is the ability to “compute” multiple things at the same time.
|BR|
Designing concurrent software isn’t that complicated but; demands another mindset the when we write software that does
one thing afer the other.

A typical example is a loop: suppose we have a sequence of numbers and we like to compute the square of each one. Most
developers will loop over those numbers, get one number, calculate the square, store it in another list, and continue.
It works, but we have also instructed the computer to do it in sequence — especially when the
task is bit more complicated, the compiler does know whether the ‘next task’ depends on the current one, and can’t
optimise it.

A better plan is to tell the compiler about the tasks; most are independently: square a number. There is also one that
has to be run at the end: combine the results into a new list. And one is bit funny: distribute the sequence-elements
over the “square-tasks” — clearly, one has to start with this one, but it can be concurrent with many others too.
|BR|
This is *not* a parallel algorithm. When not specifying the order, we allow parallel execution. We do not demand it,
sequential execution is allowed too.



Parallelisme
------------

Parallelisme_ is about executing multiple tasks (apparently) at the same time. We will focus running multiple concurrent
task (of the same program) on “as many cores as possible”.  When we assume a thousand cores, we need a thousand
independent tasks (at least) to gain maximal speed up. A thousand at any moment!
|BR|
It’s not only about doing a thousand things at the same time (that is not to complicated, for a computer), but also —
probably: mostly — about finishing a thousand times faster…

With many cores, multiple program-steps can be executed at the same time: from changing the same variable,  acces the
same memory, or compete for new memory. And when solving that, we introduce new hazards: like deadlocks_ and even
livelocks_.


Distributed
-----------

A special form of parallelisme is Distributed-Computing_: compute on many computers. Many experts consider this
as an independent field of expertise; still --as Multi-Core_ is basically “many computers on a chips”-- its there is an
analogy [#DistributedDiff]_, and we should use the know-how that is available, to design out “best ever language”.

Messages & shared-data
----------------------

Communication between two (concurrent) tasks (or processes, CPUs, computers) needs the passing of data (in one or two
direction). Roughly, there are two ways to do so:

Shared-Data
   Memory (variables) that can be written and/or read by both. As the acces is typical not acces, a bit of 


Controll
========

Models
======

Probably the oldest model to described concurrency is the Petri-net_; which has the disadvantage that is synchronous
(all tokens move at the same timeslot) -- which is a hard to implement (efficiently) on Multi-Core_.

Actors

A very introduce

--------

END

----------

.. rubric:: Footnotes

.. [#DistributedDiff]
   There a two (main) differences between Distributed-Computing_ and Multi-Core_. Firstly, all “CPUs” in
   Distributed-Computing_ are active, independent and asynchronous. There is no option to share a “core” (as
   commonly/occasionally done in Multi-process/Threaded programming); nor is there “shared memory” (one can only send
   messages over a network).
   |BR|
   Secondly, collaboration with (network based) messages is a few orders slower then (shared) memory communication. This
   makes it harder to speed-up; the delay of messaging shouldn't be bigger as the acceleration do doing thing in
   parallel.
   |BR|
   But that condition does apply to Multi-Core_ too. Although the (timing) numbers do differ.

.. _pthreads: 			https://en.wikipedia.org/wiki/Pthreads
.. _Threads:			https://en.wikipedia.org/wiki/Thread_(computing)
.. _Multi-Core:			https://en.wikipedia.org/wiki/Multi-core_processor

.. _deadlocks:			https://en.wikipedia.org/wiki/Deadlock
.. _livelocks:			https://en.wikipedia.org/wiki/Deadlock#Livelock
.. _Critical-Sections:		https://en.wikipedia.org/wiki/Critical_section
.. _Distributed-Computing:	https://en.wikipedia.org/wiki/Distributed_computing

.. _Actor-Model:		https://en.wikipedia.org/wiki/Actor_model
.. _Actor-Model-Theory:		https://en.wikipedia.org/wiki/Actor_model_theory

.. _Petri-Net:			https://en.wikipedia.org/wiki/Petri_net
