.. include:: /std/localtoc.irst

.. _ConcurrentComputingConcepts:

====================================
Concurrent Computing Concepts (BUSY)
====================================

.. post::	
   :category: Castle DesignStudy
   :tags: Castle, Concurrency, DRAFT

   Shortly, more and more cores will become available alike I described in “:ref:`BusyCores`”. Castle should
   make it easy to write code for all of them: not to keep them busy, but to maximize speed up [useCase:
   :need:`U_ManyCore`].
   |BR|
   We also discussed threads_: they do not scale well for CPU-bound (embedded) systems. And, I introduced some
   (other) concurrency abstractions; which do not always fit nicely in existing languages.

   As Castle is a new language we have the opportunity to select such a concept and incorporate it into the language ...
   |BR|
   In this blog, we explore a bit of theory. I will focus on semantics and the possibilities to implement them
   efficiently. The exact syntax will come later.

Basic terminology
=================

There are many theories available and some more practical expertise but they hardly share a common vocabulary.
For that reason, let describe some basic terms, that will be used in these blogs. As always, we use Wikipedia as common
ground, and add links for a deep-dive. 
|BR|
Again, we use ‘task’ as the most generic term of work-to-be-executed; that can be (in) a process, (on) a thread, (by) a
computer, etc.

.. include:: CCC-sidebar-concurrency.irst

Concurrent
----------

Concurrency_ is the **ability** to “compute” multiple *tasks* at the same time.
|BR|
Designing concurrent software isn’t that complicated but; demands another mindset the when we write software that does
one tasks afer the other.

A typical example is a loop: suppose we have a sequence of numbers and we like to compute the square of each one. Most
developers will loop over those numbers, get one number, calculate the square, store it in another list, and continue.
It works, but we have also instructed the computer to do it in sequence — especially when the
task is bit more complicated, the compiler does know whether the ‘next task’ depends on the current one, and can’t
optimise it.

A better plan is to tell the compiler about the tasks; most are independently: square a number. There is also one that
has to be run at the end: combine the results into a new list. And one is bit funny: distribute the sequence-elements
over the “square-tasks” — clearly, one has to start with this one, but it can be concurrent with many others too.
|BR|
This is *not* a parallel algorithm. When not specifying the order, we allow parallel execution. We do not demand it,
sequential execution is allowed too.


Parallelism
-----------

Parallelism_ is about executing multiple tasks (seemingly) at the same time. We will focus running multiple concurrent
task (of the same program) on *“as many cores as possible”*.  When we assume a thousand cores, we need a thousand
independent tasks (at least) to gain maximal speed up. A thousand at any moment!
|BR|
It’s not only about doing a thousand tasks at the same time (that is not to complicated, for a computer), but also —
probably: mostly — about finishing a thousand times faster…

With many cores, multiple program-steps can be executed at the same time: from changing the same variable,  acces the
same memory, or compete for new memory. And when solving that, we introduce new hazards: like deadlocks_ and even
livelocks_.


Distributed
~~~~~~~~~~~

A special form of parallelisme is Distributed-Computing_: compute on many computers. Many experts consider this
as an independent field of expertise; still --as Multi-Core_ is basically “many computers on a chips”-- its there is an
analogy [#DistributedDiff]_, and we should use the know-how that is available, to design out “best ever language”.


Efficient Communication
=======================

When multiple tasks run in concurrently, they have to communicate to pass data and to controll progress. Unlike in a
sequential program -- where the controll is trivial, as is sharing data-- this needs a bit of extra effort.
|BR|
There are two main approches: shared-data of message-passing; we will introduce them below.

Communication takes time, especially *wall-time* [#wall-time]_ (or real time) and may slow down computing. Therefore
communication has to be efficient. This is a hard problem and becomes harder when we have more communication, more
concurrently (& parallel) tasks, and/or those task are shorter living. Or better: is depends on the ratio
communication-time over time-between-two-communications.


Shared Memory
-------------

In this model all tasks (usually threads or process) have some shared/common memory; typically “variables”. As the acces
is asynchronous, the risk exist the data is updated “at the same time” by two or more tasks. This can lead to invalid
data and so Critical-Sections_ are needed.

This is a very basic model which assumes that there is physically memory that can be shared. In distributed systems this
is uncommon; but for threads it’s straightforward. As disadvantage of this model is that is hazardous: Even when a
single access to such a shared variable is not protected by a Critical-Section_, the whole system can break [#OOCS]_.

The advantage of shared memory is the short communication-time. The wall-time and CPU-time are roughly the same: the
time to write & read the variable, added to the (overhead) time for the critical section --  which is typical the
bigger part.

About Critical Sections
~~~~~~~~~~~~~~~~~~~~~~~
For those, who are not familiar with Critical-Sections_ and/or Semaphores, a very short intro.
|BR|
Unlike some developers presume “code-lines” are not *‘atomic’*: they can be interrupted. When using (e.g) threads_, the
“computer” can pause one thread halfway an statement, to run another one temporally and continue a millisecond later. When
that happens when writing or reading a variable and the other thread also access the same shared-memory, the result is
unpredictable. To prevent that, we need to controle the handling that variable: make it a Critical-Section_.

In essense, we have to tell the “computer” that a line (of a few lines) are *atomic*; to make access exclusive  The
the compiler will add some extra fundamental instructions (specific for that CPU-type) to assure this. A check is
inserted just before the section is entered, and the thread will be suspended when another task is using it. When
granted acces, a bit of bookkeeping is done -- so the “check” in other thread is halted). That bookkeeping is updated
and when leaving. Along with more bookkeeping to un-pauze the suspended threads.

As you can imagen, this “bookkeeping” is extra complicated on a Multi-Core_ system; some global data structure is
needed; which is a Critical-Sections in itself.
|BR|
There are many algorithms to solve this. All with the same disadvantage: it takes a bit of time -- possible by
“Spinlocking_” all other cores (for a few nanoseconds). As Critical-Sections a usually short (e.g a assignment, or a
few lines) the overhead can be (relatively) huge [#timesCPU]_!


Messages
--------

A more modern approach is Message-Passing_: a task sends some information to another; this can be a message, some data,
or an event. In all cases, there is a distinct sender and receiver -- and apparently no common/shared memory-- so no
Critical-Sections [#MPCS]_ are needed; at least not explicitly. Messages can be used by all kind of task; even in a
distributed system -- then the message (and it data) is serialised, transmitted over a network and deserialised. Which
can introduce some overhead and delay.

As you may have noticed, there is an analogy between Message-Passing_ and Events_ (in an the event-loop). They have
separate history, but are quite similar in nature. Like a “message”, the “event” is also used to share data (& controll)
to isolated “tasks”.

.. warning::

   Many people use the networking mental model when they thing about Message-Passing_, and *wrongly* assume there is
   always serialisation (and network) overhead. This is not needed for parallel cores as they typically have shared
   (physical) memory.

   Then, we can use the message abstraction at developer-level, and let the compiler will translate that it into shared
   memory instructions at processor level.
   |BR|
   Notice: As the compiler will insert the (low level) Semaphores_, the risk that a developer forgets one is gone!

There are many variant on messaging, mostly combinations some fundamental aspects

.. include:: CCC-sidebar-async.irst

(A)Synchronous
~~~~~~~~~~~~~~

Synchronous messages resembles normal function-calls. Typically a “question” is send, the call awaits the
answer-messages, and that answer is returned. This can be seen as a layer on top of the more fundamental send/receive
calls. An famous example is RPC_: the Remote Procedure Call.

Asynchronous messages are more basic: a task send a messages (to somebody else) and continues. That message can be
“data”, an “event:, a “commands” or a “query”. Only in the latter case some responds is essental. With async messages,
there is no desire that to get the answer immediately.

As an example: A task can send many queries (and/or other messages) to multiple destinations at once, then go into
*listen-mode*, and handle the replies in the order the are received (which can be different then send-order). Typically,
this speeds-up (wall) time, and is only possible with async messages. Notice: the return messages need to carry an “ID”
of the initial messages to keep track -- often that is the query itself.


(Un)Buffered
~~~~~~~~~~~~

Despide it’s is not truly a characteristic of the messages itself, messages can be *buffered*, or not. This about the
channel that transports the messages: How many messages can be stored ‘in’ the channel (often depicted a a queue). When
there is no storage at all, the writer and reader needs to rendezvous: send and receive at the same (wall) time.

This is not needed when a buffer is available. Depending on the size of the buffer, some messages can be send before
the are picked-up by the receiver.
|BR|
Note this is always asymmetric: messages need to be send before the can be read.

Blocking
~~~~~~~~
Both the writer and the reader can be blocking (or not); which is a facet of the function-call. A blocking reader it
will always return when a messages is available -- and will pauze until then.
|BR|
Also the write-call can be blocking: it will pauze until the message can be send -- e.g. the reader is available
(rendezvous) or a message-buffer is free.

When the call is non-blocking, the call will return without waiting and yield a flag whether is was successful or not.
Then, the developer will commonly “cycle” to poll for a profitable call; and let the task do some other/background work
as well.


************************************************************

.. todo::


   * Pipe : kind of data messages 


 .. todo:: All below is draft and needs work!!!!


Models
======

Probably the oldest model to described concurrency is the
(all tokens move at the same timeslot) -- which is a hard to implement (efficiently) on Multi-Core_.

Actors

Actor-Model_
Actor-Model-Theory_

A very introduce

--------

END

----------

.. rubric:: Footnotes

.. [#DistributedDiff]
   There a two (main) differences between Distributed-Computing_ and Multi-Core_. Firstly, all “CPUs” in
   Distributed-Computing_ are active, independent and asynchronous. There is no option to share a “core” (as
   commonly/occasionally done in Multi-process/Threaded programming); nor is there “shared memory” (one can only send
   messages over a network).
   |BR|
   Secondly, collaboration with (network based) messages is a few orders slower then (shared) memory communication. This
   makes it harder to speed-up; the delay of messaging shouldn't be bigger as the acceleration do doing thing in
   parallel.
   |BR|
   But that condition does apply to Multi-Core_ too. Although the (timing) numbers do differ.

.. [#wall-time]
   As reminder: We speak about *CPU-time* when we count the cycles that a core us busy; so when a core is waiting, no
   CPU-time is used. And we use *wall-time* when we time according the “the clock on the wall”.

.. [#OOCS]
   The brittleness of Critical-Sections_ can be reduced by embedding (the) (shared-) variable in an OO abstraction. By
   using *getters and *setters*, that controll the access, the biggest risk is (mostly) gone. That does not, however,
   prevent deadlocks_ nor livelocks_. Also see the note below.

.. [#MPCS]
   This is not completely correct; Message-Passing_ can be implemented on top of shared-memory. Then, the implementation
   of this (usually) OO-abstraction contains the Critical-Sections_; a bit as described in the footnote above.

.. [#timesCPU]
   And the overhead will grow when we add more cores. Firstly while more “others” have to wait (or spin), and secondly
   that the number of communications will grow with the number of cores too.  As described in the :ref:`sidebar
   <Threads-in-CPython>` in :ref:`BusyCores`, solving this can give more overhead then the speed we are aiming for.

.. _Concurrency:		https://en.wikipedia.org/wiki/Concurrency_(computer_science)
.. _parallelism:		https://en.wikipedia.org/wiki/Parallel_computing
.. _pthreads: 			https://en.wikipedia.org/wiki/Pthreads
.. _Threads:			https://en.wikipedia.org/wiki/Thread_(computing)
.. _Multi-Core:			https://en.wikipedia.org/wiki/Multi-core_processor
.. _deadlocks:			https://en.wikipedia.org/wiki/Deadlock
.. _livelocks:			https://en.wikipedia.org/wiki/Deadlock#Livelock
.. _Critical-Section:		https://en.wikipedia.org/wiki/Critical_section
.. _Critical-Sections:		Critical-Section_
.. _Semaphores:			https://en.wikipedia.org/wiki/Semaphore_(programming)
.. _Spinlocking:		https://en.wikipedia.org/wiki/
.. _Distributed-Computing:	https://en.wikipedia.org/wiki/Distributed_computing
.. _Message-Passing:		https://en.wikipedia.org/wiki/Message_passing
.. _Events:			https://en.wikipedia.org/wiki/Event_(computing)
.. _Actor-Model:		https://en.wikipedia.org/wiki/Actor_model
.. _Actor-Model-Theory:		https://en.wikipedia.org/wiki/Actor_model_theory
.. _RPC:			https://en.wikipedia.org/wiki/Remote_procedure_call
