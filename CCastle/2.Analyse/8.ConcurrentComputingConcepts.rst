.. include:: /std/localtoc.irst

.. _ConcurrentComputingConcepts:

====================================
Concurrent Computing Concepts (BUSY)
====================================

.. post::	
   :category: Castle DesignStudy
   :tags: Castle, Concurrency, DRAFT

   Sooner as we may realize even embedded systems will have many, many cores; as I described in
   “:ref:`BusyCores`”. Castle should make it easy to write code for all of them: not to keep them busy, but to maximize
   speed up [useCase: :need:`U_ManyCore`]. There I also showed that threads_ do not scale well for CPU-bound (embedded)
   systems. Last, I introduced some (more) concurrency abstractions. Some are great, but they often do not fit
   nicely in  existing languages.

   Still, as Castle is a new language we have the opportunity to select such a concept and incorporate it into the
   language ...
   |BR|
   In this blog, we explore a bit of theory. I will focus on semantics and the possibilities to implement them
   efficiently. The exact syntax will come later.

Basic terminology
*****************

There are many theories available and some more practical expertise but they hardly share a common vocabulary.
For that reason, let’s describe some basic terms, that will be used in these blogs. As always, we use Wikipedia as common
ground and add links for a deep dive.
|BR|
Again, we use ‘task’ as the most generic term for work-to-be-executed; that can be (in) a process, (on) a thread, (by) a
computer, etc.

.. include:: CCC-sidebar-concurrency.irst

Concurrent
==========

Concurrency_ is the **ability** to “compute” multiple *tasks* at the same time.
|BR|
Designing concurrent software isn’t that complicated but; demands another mindset than when we write software that does
one task after the other.

A typical example is a loop: suppose we have a sequence of numbers and we like to compute the square of each one. Most
developers will loop over those numbers, get one number, calculate the square, store it in another list, and continue.
It works, but we have also instructed the computer to do it in sequence. Especially when the task is a bit more
complicated, the compiler does know whether the ‘next task’ depends on the current one, and can’t optimize it.

A better plan is to tell the compiler about different tasks. Most are independent: square a number. There is also one
that has to be run at the end: combine the results into a new list. And one is a bit funny: distribute the elements over
the “square tasks”. Clearly one has to start with this one, but it can be concurrent with many others too.
|BR|
This is *not* a parallel algorithm. When not specifying the order, we allow parallel execution. We do not demand it,
sequential execution is allowed too.


Parallelism
===========

Parallelism_ is about executing multiple tasks (seemingly) at the same time. We will on focus running many multiple
concurrent tasks (of the same program) on *“as many cores as possible”*.  When we assume a thousand cores, we need a
thousand independent tasks (at least) to gain maximal speed up. A thousand at any moment!
|BR|
It’s not only about doing a thousand tasks at the same time (that is not too complicated, for a computer) but also —
probably: mostly — about finishing a thousand times faster…

With many cores, multiple “program lines” can be executed at the same time, which can introduce unforeseen effects:
changing the same variable, accessing the same memory, or competing for new, “free” memory. And when solving that, we
introduce new hazards: like deadlocks_ and even livelocks_.


Distributed
-----------

A special form of parallelism is Distributed-Computing_: computing on many computers. Many experts consider this
an independent field of expertise. Still --as Multi-Core_ is basically “many computers on a chip”-- it’s an
available, adjacent [#DistributedDiff]_ theory, and we should use it, to design our “best ever language”.

.. include:: CCC-sidebar-CS.irst

Efficient Communication
***********************

When multiple tasks run concurrently, they have to communicate to pass data and control progress. Unlike in a
sequential program -- where the control is trivial, as is sharing data-- this needs a bit of extra effort.
|BR|
There are two main approaches: shared-data of message-passing; we will introduce them below.

Communication takes time, especially *wall time* [#wall-time]_ (or clock time) and may slow down computing. Therefore
communication has to be efficient. This is an arduous problem and becomes harder when we have more communication, more
concurrency, more parallelism, and/or those tasks are short(er)living. Or better: it depends on the ratio between the
communication-time and the time-between-two-communications.


Shared Memory
=============

In this model all tasks (usually threads or processes) have some shared/common memory; typically “variables”. As the access
is asynchronous, the risk exists the data is updated “at the same time” by two or more tasks. This can lead to invalid
data and so Critical-Sections_ are needed.

This is a very basic model which assumes that there is physical memory that can be shared. In distributed systems this
is uncommon, but for threads it’s straightforward. A disadvantage of this model is that is hazardous: Even when a
single modifier of a shared variable is not protected by a Critical-Section_, the whole system can break [#OOCS]_.

The advantage of shared memory is the fast *communication-time*. The wall-time and CPU-time are roughly the same: the
time to write & read the variable added to the (overhead) time for the critical section --  which is typically the
bigger part.


Messages
========

A more modern approach is Message-Passing_: a task sends some information to another; this can be a message, some data,
or an event. In all cases, there is a distinct sender and receiver -- and apparently no common/shared memory-- so no
Critical-Sections [#MPCS]_ are needed; at least not explicitly. Messages are easier to use and more generic: they can be
used in single-, multi-, and many-core systems. Even distributed systems are possible -- then the message (and its data)
is serialised, transmitted over a network, and deserialised.

As you may have noticed, there is an analogy between Message-Passing_ and Events_ (in an event-loop). They have separate
histories but are quite similar in nature. Like a “message”, the “event” is also used to share data (& control) to
isolated “tasks”.

.. warning::

   Many people use the networking mental model when they think about Message-Passing_, and *wrongly* assume there is
   always serialisation (and network) overhead. This is not needed for parallel cores as they typically have shared
   (physical) memory.

   Then, we can use the message abstraction at developer-level, and let the compiler translate that into shared
   memory instructions for the processor level.
   |BR|
   Notice: As the compiler will insert the (low level) Semaphores_, the risk that a developer forgets one is gone!

Messaging Aspects
-----------------

There are many variant on messaging, mostly combinations some fundamental aspects. Let mentions some basic ones.

.. include:: CCC-sidebar-async.irst

(A)Synchronous
~~~~~~~~~~~~~~

**Synchronous** messages resembles normal function-calls. Typically a “question” is send, the call awaits the 
answer-messages, and that answer is returned. This can be seen as a layer on top of the more fundamental send/receive
calls. An famous example is RPC_: the Remote Procedure Call.

**Asynchronous** messages are more basic: a task send a messages (to somebody else) and continues. That message can be
“data”, an “event:, a “commands” or a “query”. Only in the latter case some responds is essental. With async messages,
there is no desire that to get the answer immediately.

As an example: A task can send many queries (and/or other messages) to multiple destinations at once, then go into
*listen-mode*, and handle the replies in the order the are received (which can be different then send-order). Typically,
this speeds-up (wall) time, and is only possible with async messages. Notice: the return messages need to carry an “ID”
of the initial messages to keep track -- often that is the query itself.


(Un)Buffered
~~~~~~~~~~~~

Despide it’s is not truly a characteristic of the messages itself, messages can be *buffered*, or not. This about the
channel that transports the messages: How many messages can be stored ‘in’ the channel (often depicted a a queue). When
there is no storage at all, the writer and reader needs to rendezvous: send and receive at the same (wall) time.

This is not needed when a buffer is available. Depending on the size of the buffer, some messages can be send before
the are picked-up by the receiver.
|BR|
Note: this is always asymmetric; messages need to be send before the can be read.

(Non-) Blocking
~~~~~~~~~~~~~~~

Both the writer and the reader can be *blocking* (or not); which is a facet of the function-call. A blocking reader it
will always return when a messages is available -- and will pauze until then.
|BR|
Also the write-call can be blocking: it will pauze until the message can be send -- e.g. the reader is available
(rendezvous) or a message-buffer is free.

When the call is non-blocking, the call will return without waiting and yield a flag whether is was successful or not.
Then, the developer will commonly “cycle” to poll for a profitable call; and let the task do some other/background work
as well.

Uni/Bi-Directional, Broadcast
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Messages --or actually the channel that transport them-- can be *unidirectional*: from sender to receiver only;
*bidirectional*: both sides can send and receive; or *broadcasted*: one message is send to many receivers [#anycast]_.

Reliability & Order
~~~~~~~~~~~~~~~~~~~

Especially when studying “network messages”, we have to consider Reliability_ too. Many developers assume that a send
message is always received and that when multiple messages are sent, they are received in the same order. In most,
traditional --single-core-- applications this is always true. With networking applications, this is not always
the case. Messages can get lost, received out of order, or even read twice. Although it is always possible to add a
“reliability layer”.
|BR|
Such a layer makes writing the application easier but introduces overhead. And therefore not always the right solution.

In Castle, we have “active components”: many cores are running parallel, all doing a part of the overall (concurrent)
program. This resembles a networking application -- even while there is no real network -- where at least three nodes
are active.

This is a bit more complicated, so let us start with an example. Say, we have 3 components ``A``, ``B1``, and
``B2``. All are connected to all others. We assume that messages are unbuffered, non-blocking, never got lost, and that
two messages over the same channel are never out-of-order. Sound simple, isn’t it?
|BR|
Now state that ``A`` send a message (`m1`) to ``B1`` and then one (`m2`) to ``B1``. The “B components”  will --on
receiving a message from ``A`` -- send a short message to the other one (`m3` and `m4`). And that message triggers
(again both in ``B1`` and ``B2``) to send an answer to ``A``; so `m5` and `m6`.

Now the question is: in which order those answers (in ``A``) are received?
|BR|
The real answer is: you don’t know!
|BR|
It’s clear that ``A`` will get `m5` and `m6` -- given that all messages (aka channels) are reliable. But there are many
ways those messages may receive in the opposite order. Presumably, even in more ways, than you can imagine. For example,
``B1`` might processes `m4` before it process `m1`! This can happen when channel ``A->B1`` is *slow*, or when ``B2``
gets CPU-time before ``B1``, or...

When we add buffering, more connected components, etc this *“network”* acts less reliable than we might aspect (even
though each message is reliable). When we add some real-time demands (see below), the ability to use/model a solution
using an unreliable message becomes attractive ...
|BR|
It’s not that you should always favor unreliable, out-of-order messages. Without regard, No! We are designing a new
language, however --one that should run efficiently on thousands of core, in a real-time embedded system-- then the
option to utilize them may be beneficial.


.. hint::

   As a simple example to demonstrate the advantage of a “unreliable connection”, lets consider an audio (bidirectional)
   connection, that is not 100% reliable.
   |BR|
   When we use it “as is”, there will be a bit of noise, and even some hick-ups. For most people this is acceptable,
   when needed they will use phrases  as “can you repeat that?”.

   To make that connection reliable, we need checksums, low-level conformation message, and once in a while send a
   message again. To make it an audio-stream all messages will be delayed a bit --so that an automatically resend
   messages can be inserted. This is common in a (unidirectional) POD-cast kind of connection.

   For a (bidirectional) conversation, this buffering is often not satisfactory. As it makes the connection *slow*.


Some examples
-------------

In the section below, we mention a few, everyday message-passing systems, to shed light on the theoretical features.

Pipes
~~~~~

The famous *Unix Pipes* are unidirectional, reliable, blocking, asynchronous, buffered, non-networking **data-only**
messages. The (“stdout”) output of one process is fed as input to (one) other process. It’s data only, in one direction
-- but the controll can in two directions: when the second (receiving) process can’t process the data (and the buffers
becoming full), the first process can be slowed down (although this a not well know feature).

It’s also an example of a quite implicit channel: the programmer (of both programs) have nothing (to little) to do
extra, to make it possible.



------------------------

.. todo::


   * Pipe : kind of data messages 


 .. todo:: All below is draft and needs work!!!!


Models
******

Probably the oldest model to described concurrency is the
(all tokens move at the same timeslot) -- which is a hard to implement (efficiently) on Multi-Core_.

Actors

Actor-Model_
Actor-Model-Theory_

A very introduce

--------

END

----------

.. rubric:: Footnotes

.. [#DistributedDiff]
   There a two (main) differences between Distributed-Computing_ and Multi-Core_. Firstly, all “CPUs” in
   Distributed-Computing_ are active, independent and asynchronous. There is no option to share a “core” (as
   commonly/occasionally done in Multi-process/Threaded programming); nor is there “shared memory” (one can only send
   messages over a network).
   |BR|
   Secondly, collaboration with (network based) messages is a few orders slower then (shared) memory communication. This
   makes it harder to speed-up; the delay of messaging shouldn't be bigger as the acceleration do doing thing in
   parallel.
   |BR|
   But that condition does apply to Multi-Core_ too. Although the (timing) numbers do differ.

.. [#wall-time]
   As reminder: We speak about *CPU-time* when we count the cycles that a core us busy; so when a core is waiting, no
   CPU-time is used. And we use *wall-time* when we time according the “the clock on the wall”.

.. [#OOCS]
   The brittleness of Critical-Sections_ can be reduced by embedding (the) (shared-) variable in an OO abstraction. By
   using *getters and *setters*, that controll the access, the biggest risk is (mostly) gone. That does not, however,
   prevent deadlocks_ nor livelocks_. Also see the note below.

.. [#MPCS]
   This is not completely correct; Message-Passing_ can be implemented on top of shared-memory. Then, the implementation
   of this (usually) OO-abstraction contains the Critical-Sections_; a bit as described in the footnote above.

.. [#timesCPU]
   And the overhead will grow when we add more cores. Firstly while more “others” have to wait (or spin), and secondly
   that the number of communications will grow with the number of cores too.  As described in the :ref:`sidebar
   <Threads-in-CPython>` in :ref:`BusyCores`, solving this can give more overhead then the speed we are aiming for.

.. [#anycast]
   Broadcasting_ is primarily know from “network messages”; where is has many variants -- mostly related to the
   physical network abilities, and the need to save bandwith. As an abstraction, they can be used in “software messages”
   (aka message passing) too.

.. _Concurrency:		https://en.wikipedia.org/wiki/Concurrency_(computer_science)
.. _parallelism:		https://en.wikipedia.org/wiki/Parallel_computing
.. _pthreads: 			https://en.wikipedia.org/wiki/Pthreads
.. _Threads:			https://en.wikipedia.org/wiki/Thread_(computing)
.. _Multi-Core:			https://en.wikipedia.org/wiki/Multi-core_processor
.. _deadlocks:			https://en.wikipedia.org/wiki/Deadlock
.. _livelocks:			https://en.wikipedia.org/wiki/Deadlock#Livelock
.. _Critical-Section:		https://en.wikipedia.org/wiki/Critical_section
.. _Critical-Sections:		Critical-Section_
.. _Semaphores:			https://en.wikipedia.org/wiki/Semaphore_(programming)
.. _Spinlocking:		https://en.wikipedia.org/wiki/
.. _Distributed-Computing:	https://en.wikipedia.org/wiki/Distributed_computing
.. _Message-Passing:		https://en.wikipedia.org/wiki/Message_passing
.. _Events:			https://en.wikipedia.org/wiki/Event_(computing)
.. _Actor-Model:		https://en.wikipedia.org/wiki/Actor_model
.. _Actor-Model-Theory:		https://en.wikipedia.org/wiki/Actor_model_theory
.. _RPC:			https://en.wikipedia.org/wiki/Remote_procedure_call
.. _Broadcasting:		https://en.wikipedia.org/wiki/Broadcasting_(networking)
.. _Reliability:		https://en.wikipedia.org/wiki/Reliability_(computer_networking)
