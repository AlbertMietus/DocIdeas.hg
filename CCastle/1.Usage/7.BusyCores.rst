.. include:: /std/localtoc.irst

.. _BusyCores:

======================
Keep those cores busy!
======================

.. post::  2022/06/26
   :category: Castle, Usage
   :tags: Castle, Concurrency

   I always claim that most computers will have 1024 or more cores before my retirement. And that most embedded systems
   will have even more, a decade later. However, it’s not easy to write technical software for those massive-parallel
   embedded computers; not with most, current languages -- simple because a developer has to put in too many details.
   Accordingly, the “best, ever” programming language should facilitate and support “natural concurrency”.

   In Castle, you can easily write code that can run efficiently on thousands of cores.

Why do we get more and more cores?
==================================

Moore `observed already in 1965 <https://en.wikipedia.org/wiki/Moore's_law>`_ that the number of transistors on a chip
doubles every 2 years. That made memory cheaper and computers faster. For years we have seen that: CPUs have grown from
4-bit to 8, 16, 32, and now 64-bit; and the ‘clock-frequently’ has risen into the giga-hertz. This comes (almost) for
free when the *‘gate-size’ dropped* -- roughly, a ‘gate’ is the smallest “pixel” when a chip is manufactured. By
shrinking the gates one can put more transistors on a chip and at the same time those chips become faster and use less
power --this comes automatically; as you may remember from your (advanced) physics lessons.

But there are limits: transistors can’t switch much faster than a few billion times a second. When we crossed that
border, just making gates smaller didn't increase the CPU’s speed. But there are other tricks; like huge caches very
close to (or ‘in’) the CPU. Again it made computers faster, without much work for the SW developer.
Another limit is the number of transistors you can utilize in a CPU (or the cache). One can double the number of
transistors almost without limit, but does it again double your CPU-speed? *No*, not anymore, that threshold is passed a
few years back.

A new trick started at the start of this century: put more CPUs onto *one* chip. Essentially that is what
Multi-Core_ does: every core is fundamentally an independent CPU - but as we used that word already, we use ‘core’ for
those copies. The amount of work a chip can do with two cores doubles — at least potentially. Using Moore’s low, the
number of cores will double be every 18-24 months; or 30 times more in a decade. Currently, many (mobile phone) SoCs
already have more than 10 cores — when counting both GPUs & CPUs. That is 300 in 10 years, and 9-thousand in 20 years!

Programming many cores
======================

.. include:: BusyCores-sidebar-CPython_threads.irst

Although the promise of Multi-Core_ is an unlimited expansion of speed, it requires us to adapt our programming
abilities. Transitional “sequential” programs take for granted that the CPU can do only one thing at a time; at many
levels. With Multi-Core_ this fundamentally changed. There is no speed-up when we don’t use all cores. This is not much
of an issue for a low number of cores; there are plenty of other/background tasks. Even ‘single threaded’ code will 
run faster when “your core” isn’t interrupted; when those auxiliary processes can run in parallel on another core.

When the number of cores rises this does not scale; more and more cores become idle. Now, your code has to use both
concurrency_ and parallelism_. But also handle Critical-Sections_, Semaphores_ (and friends) to  synchronize  tasks.


Threading
---------

Threads have become popular to implement parallelism; but have there drawbacks They originated (long, long back) in
“realtime & embedded” programming; when those systems didn’t have a OS. Other computers (mostly unix/posix) used
processes (and Windows/Dos was not existing, or single threaded). Mid 199x, threads become available as “real-time
extensions” on processed; initially within a single process — and so running on a single core. This was long before
Multi-Core_ hardware become available; and many studies exist on how to combine threads & process — should the kernel
handle that or not, and how to avoid overhead.  Nowadays this discussion is void, as we expect that threads can run
really in parallel, on multiple cores



Flaws due concurrency
=====================

* Overhead: slower ipv faster
* Heisenbugs_



Castle activates all cores
==========================

An “*ideal language*” should make it easy to distribute the load over many processors. That should come
automatically; without the developer has to put a load of effort in. That is also one of the aspect :ref:`CC` is
enabling --and as Castle supports :ref:`CC` (and is the best language, ever -- so quite close to “ideal”)-- the language
should be designed to enable this.

.. use:: In castle is easy to use thousands of cores
   :ID: U_ManyCore

   A “CC Castle” program can run on many cores, without the developer needs to describe “manually” how to do that.

   At the same time it should run efficiently. The goals is **not** to keep all cores *busy*, but to use (all) cores to
   gain maximal speed-up.

Distributing tasks over multiple cores (or even distributed computers [#web]_ is not new. In :ref:`BC-concepts` we
analyse some, before we design & describe Castle’s :ref:`XXX`.

----------

.. rubric:: Footnotes

.. [#python27-outdated]
   Both the Jython and Iron-Python implementations are effectively dead as there is only support for Python-2.7. That
   however is ortogonal on the (alternative) way it handles threads.

.. [#GIL-speed]
   Using the GIL was not a bad option. It made code fast, both for non-threading as for threading!
   |BR|
   Many attempt have made to “remove the GIL”: all failed. Not because removing the GIL (itself) is hard, but as the
   replacements made the code slower. Always for single-threaded applications; but sometimes the extra overhead was even
   bigger then the speed-up when using all cores. And even that was mainly in the “first tackle”, let it be a lesson
   for “Busy Cores”:

   *It not about using all cores, it about the speed-up!*

.. [#CS-link]
   See :ref:`BC-concepts` for more on Critical-Sections_ and other well-know concepts and how they relate.


.. [#web]
   A trivial example of dividing work over multiple computers (and so cores) is the “Web”. You have at least two
   concurrent (active) components: the server and the brouwer. Usually there a more. By example, the server-side run’s
   the application- a web-, and database-server. And “the web itself” has many active components, like routers, ect. And
   all are programmed in a style that the developer doesn't need to know all those details(!).

   
.. _Multi-Core:			https://en.wikipedia.org/wiki/Multi-core_processor
.. _Concurrency:		https://en.wikipedia.org/wiki/Concurrency_(computer_science)
.. _parallelism:		https://en.wikipedia.org/wiki/Parallel_computing
.. _Critical-Sections:		https://en.wikipedia.org/wiki/Critical_section
.. _Semaphores:			https://en.wikipedia.org/wiki/Semaphore_(programming)
.. _Threads:			https://en.wikipedia.org/wiki/Thread_(computing)
.. _Heisenbugs:			https://en.wikipedia.org/wiki/Heisenbug
.. _

